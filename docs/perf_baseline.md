# Helios.Embed v1.0.0 - Performance Baseline & Scalability Report

This document presents the official performance baseline for the `Helios.Embed` v1.0.0 module. The data was generated by the `tests/benchmark_scalability.py` script and is stored in `benchmark_outputs/scalability_results.csv`.

**Reference Hardware:** NVIDIA T4 GPU (as found in Google Colab)
**Software Versions:** See `SUPPORT_MATRIX.md`

---

## 1. Executive Summary

The `Helios.Embed` engine demonstrates **robust, predictable, and near-linear performance scaling** across all key operational parameters (`N`, `D`, `m`). The ATen-backed C++/CUDA implementation is stable and efficient, with no evidence of quadratic or other unfavorable scaling bottlenecks. The engine's runtime and memory usage are primarily driven by the underlying `torch.cdist` and `torch.matmul` operations, which are highly optimized.

## 2. Performance Scaling Analysis

The following plots, generated by our benchmark suite, characterize the performance envelope of the `compute_rkhs_embedding` function.

![Performance and Scalability Envelope Plots](https://raw.githubusercontent.com/irbsurfer/Helios/main/helios_embed/benchmark_outputs/scalability_plots.png)

*(Note: To display this image correctly on GitHub Pages, the image file `scalability_plots.png` must be committed to the repository at the specified path, e.g., in a `benchmark_outputs` directory that is included in the deployed site.)*

### 2.1. Time vs. Number of Vectors (N)

*   **Observation:** Build time scales in a **near-linear** fashion with the number of input vectors `N`. The log-log plot shows a slope slightly greater than 1, consistent with the theoretical complexity of $O(N \cdot m \cdot D)$ for the dominant distance calculation step.
*   **Conclusion:** The engine is highly scalable for large datasets.

### 2.2. Time vs. Feature Dimension (D)

*   **Observation:** Build time scales **linearly** with the input feature dimension `D`.
*   **Conclusion:** The performance cost of using higher-dimensional embeddings is predictable and manageable.

### 2.3. Time vs. Number of Landmarks (m)

*   **Observation:** Build time scales **linearly** with the number of landmarks `m`.
*   **Conclusion:** This provides a predictable "dial" for the speed-vs-accuracy trade-off. Doubling the number of landmarks to increase feature quality will approximately double the build time.

### 2.4. Peak Memory vs. Number of Vectors (N)

*   **Observation:** The additional memory allocated during the build process scales **perfectly linearly** with the number of input vectors `N`.
*   **Conclusion:** The engine's memory footprint is predictable and efficient, with no hidden quadratic memory costs. This is critical for deployment on memory-constrained hardware.

## 3. Raw Performance Data

The following table summarizes the key results from the scalability benchmark. This data serves as the **official v1.0.0 performance baseline** for all future regression testing.

| N | D | m | build_time_ms | build_mem_mb |
|---|---|---|---|---|
| 4096 | 128 | 256 | 1.1325 | 16.03 |
| 8192 | 768 | 256 | 3.5028 | 48.13 |
| 16384 | 384 | 512 | 7.3065 | 128.00 |
| ... | ... | ... | ... | ... |

*(Note: This is a representative sample. The full baseline is located in `benchmark_outputs/scalability_results.csv`.)*

## 4. Regression Policy

As stated in `PERFORMANCE_POLICY.md`, any future code change that causes a regression of **greater than 5%** against this baseline must be rejected unless accompanied by a compelling and documented justification.